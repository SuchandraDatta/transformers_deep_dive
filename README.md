<h1>Transformers Deep Dive</h1>

<p>
Hello world :wave:
<br/>
I often spend my weekends exploring what transformer-based architectures are doing under the hood and in this repo I'm documenting my findings.


<h3>Summary of content</h3>
<ul>
<h4>BERT embeddings</h4>
<li>First embedding layer (Embedding table )</li>
<li>Last 4 layers summed up embeddings</li>
<li>Comparing the embeddings from the two different strategies</li>
<li>Sentence embeddings</li>
<li>Animated - how embedding values are changing as it passes through each of the 12 layers</li>
<br/>
<h4>BERT weights</h4>
<li>Weights distribution for query, key, values</li>
<li>Weights distribution for other layers</li>
<li>Attention score vs token per attention head</li>
<li>Attention score vs token per layer</li>
<li>Observations</li>
</ul>
</p>
